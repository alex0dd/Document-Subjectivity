{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document_Subjectivity_Experiment_1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "cNvVyxgEa56v"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ12kxS5ZK3D"
      },
      "source": [
        "Experiment 1\n",
        "\n",
        "Perform document classification based on document embeddings. Document embeddings can be computed by some sentence embedder model and aggregated together by some aggregation procedure (e.g. mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNvVyxgEa56v"
      },
      "source": [
        "# Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW4ai4Yea8Fi"
      },
      "source": [
        "!git clone https://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:x-oauth-basic@github.com/alexpod1000/Document-Subjectivity.git\n",
        "%cd Document-Subjectivity/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNyz0em8ao7M"
      },
      "source": [
        "# get the dataset folder\n",
        "!git clone https://github.com/francescoantici/SubjectivITA.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gvjT0dpLCQW"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP4ne673awgC"
      },
      "source": [
        "# Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0jjaqB5yId5"
      },
      "source": [
        "POSSIBLE_EMBEDDERS = [\n",
        "    \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    \"paraphrase-multilingual-mpnet-base-v2\",\n",
        "    \"distiluse-base-multilingual-cased-v1\"\n",
        "]\n",
        "SENTENCE_EMBEDDER_MODEL = POSSIBLE_EMBEDDERS[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayuHthdvcdCc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "# Sklearn imports\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5kuP2Ca3Xzi"
      },
      "source": [
        "def get_sentences(split):\n",
        "    \"\"\"\n",
        "    Given a split (train, val, test), load the appropriate sentences file.\n",
        "\n",
        "    Args:\n",
        "        split (str): string indicating the split.\n",
        "    Returns:\n",
        "        df: dataframe with rows representing sentences from articles from\n",
        "        the given split.\n",
        "    \"\"\"\n",
        "    to_keep = [\"ID_ARTICOLO\", \"ID_FRASE\", \"FRASE\", \"TAG_FRASE\", \"TAG_ARTICOLO\", \"FONTE\"]\n",
        "    tag_mapper = {\"SOG\": 0, \"OGG\": 1}\n",
        "    df = pd.read_csv(\n",
        "        \"SubjectivITA/datasets/sentences/sentences{}.csv\".format(split.capitalize())\n",
        "    )\n",
        "    df[\"TAG_FRASE\"] = df[\"TAG_FRASE\"].replace(tag_mapper)\n",
        "    df[\"TAG_ARTICOLO\"] = df[\"TAG_ARTICOLO\"].replace(tag_mapper)\n",
        "    df[\"FONTE\"] = df[\"FONTE\"].astype('category').cat.codes\n",
        "    return df[to_keep]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmZyGjPwDPR0"
      },
      "source": [
        "def load_split_ids(split):\n",
        "    \"\"\"\n",
        "    Given a split (train, val, test), loads all the article ids for that split.\n",
        "\n",
        "    Args:\n",
        "        split (str): string indicating the split\n",
        "    Returns:\n",
        "        article_ids (List[str]): list containing article ids for the split.\n",
        "    \"\"\"\n",
        "    df_articles = pd.read_csv(\n",
        "        \"SubjectivITA/datasets/articles/articles{}.csv\".format(split.capitalize())\n",
        "    )\n",
        "    df_articles[\"ID_ARTICOLO\"] = df_articles[\"ID_ARTICOLO\"].astype(np.int32)\n",
        "    return df_articles[\"ID_ARTICOLO\"].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxqJ5WK4DGux"
      },
      "source": [
        "# load the article ids for train and test splits\n",
        "articles_ids_train = load_split_ids(\"train\")\n",
        "articles_ids_test = load_split_ids(\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsugc53VE9hU"
      },
      "source": [
        "def build_articles_embeddings_data(df, model, aggr_fn = partial(np.mean, axis=0)):\n",
        "    \"\"\"\n",
        "    Given a dataframe, a document embedding model and an embedding aggregation\n",
        "    function, returns embeddings and subjectivity label as a numpy array.\n",
        "\n",
        "    Args:\n",
        "        df: dataframe containing sentences and their corresponding article id.\n",
        "        model: a document embedding model.\n",
        "        aggr_fn: function to combine document embeddings into a single representation.\n",
        "\n",
        "    Returns:\n",
        "        X (np.array): matrix in the shape of (n_articles, emb_dim)\n",
        "        y (np.array): vector of article tag for each article\n",
        "    \"\"\"\n",
        "    # Get all the sentences in the dataframe as a list\n",
        "    sentences_list = df[\"FRASE\"].tolist()\n",
        "    # Embed all the sentences in the dataframe at once and return them as a list\n",
        "    # inside the dataframe.\n",
        "    df[\"EMB_FRASE\"] = model.encode(\n",
        "        sentences_list, convert_to_tensor=True\n",
        "    ).cpu().numpy().tolist()\n",
        "    # Helper dictionary for a pandas dataframe\n",
        "    articles_dataset = {}\n",
        "    # Divide articles by groups\n",
        "    article_groups = df.groupby(['ID_ARTICOLO'])\n",
        "    # For each unique article\n",
        "    for article_group in article_groups:\n",
        "        article_id = article_group[0]\n",
        "        # All the sentences for the given article_id\n",
        "        df_by_article_id = article_group[1]\n",
        "        sentence_embeddings = np.array(df_by_article_id[\"EMB_FRASE\"].tolist())\n",
        "        # Get embeddings for a single document (from sentence embeddings)\n",
        "        document_embedding = aggr_fn(sentence_embeddings)\n",
        "        articles_dataset[article_id] = {\n",
        "            \"document_emb\": document_embedding, \n",
        "            \"article_tag\": df_by_article_id[\"TAG_ARTICOLO\"].tolist()[0]\n",
        "        }\n",
        "    articles_df = pd.DataFrame(articles_dataset).T\n",
        "    # Convert to numpy\n",
        "    X = np.array(articles_df[\"document_emb\"].tolist(), dtype=np.float32)\n",
        "    y = np.array(articles_df[\"article_tag\"], dtype=np.float32)\n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JntbWoeG_ZA"
      },
      "source": [
        "# Load the sentence dataframe for each split\n",
        "df_train = get_sentences(\"train\")\n",
        "df_val = get_sentences(\"val\")\n",
        "df_test = get_sentences(\"test\")\n",
        "# Combine all the splits into a single dataframe\n",
        "df_all = pd.concat([df_train, df_val, df_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU-hfQaDFMX0"
      },
      "source": [
        "# redistribute data according to articles splits\n",
        "df_train = df_all.query('ID_ARTICOLO in @articles_ids_train')\n",
        "df_test = df_all.query('ID_ARTICOLO in @articles_ids_test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVxXKhrjGISX"
      },
      "source": [
        "# Get document embedder\n",
        "sentence_embedder_model = SentenceTransformer(SENTENCE_EMBEDDER_MODEL)\n",
        "aggr_fn = partial(np.mean, axis=0)\n",
        "# Build the datasets for classical model\n",
        "X_train, y_train = build_articles_embeddings_data(df_train, sentence_embedder_model, aggr_fn=aggr_fn)\n",
        "X_test, y_test = build_articles_embeddings_data(df_test, sentence_embedder_model,  aggr_fn=aggr_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzMmIZupHKOU"
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7f6DN-YEhOY"
      },
      "source": [
        "weights = compute_class_weight(class_weight = 'balanced', classes = [0.0, 1.0], y = y_train)\n",
        "class_weights = {0 : weights[0], 1: weights[1]}\n",
        "print(f\"Class weights are {class_weights}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brJlQ54mEbIa"
      },
      "source": [
        "classifier_models_list = {\n",
        "    \"svm\": LinearSVC(class_weight=class_weights),\n",
        "    \"logistic\": LogisticRegression(class_weight=class_weights),\n",
        "    \"random-forest\": RandomForestClassifier(class_weight=class_weights),\n",
        "    \"decision-tree\": DecisionTreeClassifier(class_weight=class_weights)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Vz3vpRXasS"
      },
      "source": [
        "for model_name, classifier_model in classifier_models_list.items():\n",
        "    classifier_model.fit(X_train, y_train)\n",
        "    print(f\"Results for model {model_name}\")\n",
        "    print(classification_report(y_test, classifier_model.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrJ_t1kd02Qf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}