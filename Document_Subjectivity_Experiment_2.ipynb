{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document_Subjectivity_Experiment_2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "cNvVyxgEa56v"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ12kxS5ZK3D"
      },
      "source": [
        "Experiment 2\n",
        "\n",
        "Perform document classification based on document embeddings and statistical features from the articles. \n",
        "\n",
        "We expect that statistical features for documents, namely ratio of subjective/objective sentences, is already a strong predictor for the overall document class. Hence why combining statistical features with document embeddings, should in principle be beneficial in order to be a useful solution (we don't want to waste computational resources to degrade the model's metrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNvVyxgEa56v"
      },
      "source": [
        "# Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW4ai4Yea8Fi"
      },
      "source": [
        "!git clone https://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:x-oauth-basic@github.com/alexpod1000/Document-Subjectivity.git\n",
        "%cd Document-Subjectivity/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNyz0em8ao7M"
      },
      "source": [
        "# get the dataset folder\n",
        "!git clone https://github.com/francescoantici/SubjectivITA.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gvjT0dpLCQW"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP4ne673awgC"
      },
      "source": [
        "# Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0jjaqB5yId5"
      },
      "source": [
        "POSSIBLE_EMBEDDERS = [\n",
        "    \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    \"paraphrase-multilingual-mpnet-base-v2\",\n",
        "    \"distiluse-base-multilingual-cased-v1\"\n",
        "]\n",
        "SENTENCE_EMBEDDER_MODEL = POSSIBLE_EMBEDDERS[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayuHthdvcdCc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "# Sklearn imports\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5kuP2Ca3Xzi"
      },
      "source": [
        "def get_sentences(split):\n",
        "    \"\"\"\n",
        "    Given a split (train, val, test), load the appropriate sentences file.\n",
        "\n",
        "    Args:\n",
        "        split (str): string indicating the split.\n",
        "    Returns:\n",
        "        df: dataframe with rows representing sentences from articles from\n",
        "        the given split.\n",
        "    \"\"\"\n",
        "    to_keep = [\"ID_ARTICOLO\", \"ID_FRASE\", \"FRASE\", \"TAG_FRASE\", \"TAG_ARTICOLO\", \"FONTE\"]\n",
        "    tag_mapper = {\"SOG\": 0, \"OGG\": 1}\n",
        "    df = pd.read_csv(\n",
        "        \"SubjectivITA/datasets/sentences/sentences{}.csv\".format(split.capitalize())\n",
        "    )\n",
        "    df[\"TAG_FRASE\"] = df[\"TAG_FRASE\"].replace(tag_mapper)\n",
        "    df[\"TAG_ARTICOLO\"] = df[\"TAG_ARTICOLO\"].replace(tag_mapper)\n",
        "    df[\"FONTE\"] = df[\"FONTE\"].astype('category').cat.codes\n",
        "    return df[to_keep]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmZyGjPwDPR0"
      },
      "source": [
        "def load_split_ids(split):\n",
        "    \"\"\"\n",
        "    Given a split (train, test), loads all the article ids for that split.\n",
        "\n",
        "    Args:\n",
        "        split (str): string indicating the split\n",
        "    Returns:\n",
        "        article_ids (List[str]): list containing article ids for the split.\n",
        "    \"\"\"\n",
        "    df_articles = pd.read_csv(\n",
        "        \"SubjectivITA/datasets/articles/articles{}.csv\".format(split.capitalize())\n",
        "    )\n",
        "    df_articles[\"ID_ARTICOLO\"] = df_articles[\"ID_ARTICOLO\"].astype(np.int32)\n",
        "    return df_articles[\"ID_ARTICOLO\"].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxqJ5WK4DGux"
      },
      "source": [
        "# load the article ids for train and test splits\n",
        "articles_ids_train = load_split_ids(\"train\")\n",
        "articles_ids_test = load_split_ids(\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsugc53VE9hU"
      },
      "source": [
        "def get_statistical_data(df, train_sources=None):\n",
        "    \"\"\"\n",
        "    Given an article dataframe, return a list containing its statistical features.\n",
        "    All the article sources that weren't in the initial training dataset will be\n",
        "    mapped to an \"unknown\" source.\n",
        "    \"\"\"\n",
        "    obj_ratio = sum(df[df[\"TAG_FRASE\"]==1][\"TAG_FRASE\"])/len(df)\n",
        "    if train_sources is not None:\n",
        "        # map source to a one hot\n",
        "        source_id = list(df[\"FONTE\"])[0]\n",
        "        if source_id not in train_sources:\n",
        "            source_id = 0\n",
        "        else:\n",
        "            # we assume \"missing\" category to be at index 0\n",
        "            source_id = train_sources.index(source_id) + 1\n",
        "        source_encoding = np.zeros(len(train_sources) + 1, dtype=np.float32)\n",
        "        source_encoding[source_id] = 1.0\n",
        "\n",
        "        # add batch dimension\n",
        "        return np.hstack([obj_ratio, source_encoding])\n",
        "    else:\n",
        "        return np.array([obj_ratio])\n",
        "\n",
        "def build_articles_embeddings_data(df, train_sources, model, aggr_fn = partial(np.mean, axis=0)):\n",
        "    \"\"\"\n",
        "    Given a dataframe, a document embedding model and an embedding aggregation\n",
        "    function, returns embeddings and subjectivity label as a numpy array.\n",
        "\n",
        "    Args:\n",
        "        df: dataframe containing sentences and their corresponding article id.\n",
        "        train_sources: ordered set containing article sources for training data.\n",
        "        model: a document embedding model.\n",
        "        aggr_fn: function to combine document embeddings into a single representation.\n",
        "\n",
        "    Returns:\n",
        "        X (np.array): matrix in the shape of (n_articles, emb_dim)\n",
        "        y (np.array): vector of article tag for each article\n",
        "    \"\"\"\n",
        "    # Get all the sentences in the dataframe as a list\n",
        "    sentences_list = df[\"FRASE\"].tolist()\n",
        "    # Embed all the sentences in the dataframe at once and return them as a list\n",
        "    # inside the dataframe.\n",
        "    df[\"EMB_FRASE\"] = model.encode(\n",
        "        sentences_list, convert_to_tensor=True\n",
        "    ).cpu().numpy().tolist()\n",
        "    # Helper dictionary for a pandas dataframe\n",
        "    articles_dataset = {}\n",
        "    # Divide articles by groups\n",
        "    article_groups = df.groupby(['ID_ARTICOLO'])\n",
        "    # For each unique article\n",
        "    for article_group in article_groups:\n",
        "        article_id = article_group[0]\n",
        "        # All the sentences for the given article_id\n",
        "        df_by_article_id = article_group[1]\n",
        "        # Document statistical features\n",
        "        doc_stat = get_statistical_data(df_by_article_id, train_sources)\n",
        "\n",
        "        sentence_embeddings = np.array(df_by_article_id[\"EMB_FRASE\"].tolist())\n",
        "        # Get embeddings for a single document (from sentence embeddings)\n",
        "        document_embedding = aggr_fn(sentence_embeddings)\n",
        "        articles_dataset[article_id] = {\n",
        "            \"document_emb\": np.hstack([document_embedding, doc_stat]), \n",
        "            \"article_tag\": df_by_article_id[\"TAG_ARTICOLO\"].tolist()[0]\n",
        "        }\n",
        "    articles_df = pd.DataFrame(articles_dataset).T\n",
        "    # Convert to numpy\n",
        "    X = np.array(articles_df[\"document_emb\"].tolist(), dtype=np.float32)\n",
        "    y = np.array(articles_df[\"article_tag\"], dtype=np.float32)\n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JntbWoeG_ZA"
      },
      "source": [
        "# Load the sentence dataframe for each split\n",
        "df_train = get_sentences(\"train\")\n",
        "df_val = get_sentences(\"val\")\n",
        "df_test = get_sentences(\"test\")\n",
        "# Combine all the splits into a single dataframe\n",
        "df_all = pd.concat([df_train, df_val, df_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU-hfQaDFMX0"
      },
      "source": [
        "# redistribute data according to articles splits\n",
        "df_train = df_all.query('ID_ARTICOLO in @articles_ids_train')\n",
        "df_test = df_all.query('ID_ARTICOLO in @articles_ids_test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVxXKhrjGISX"
      },
      "source": [
        "# Get document embedder\n",
        "sentence_embedder_model = SentenceTransformer(SENTENCE_EMBEDDER_MODEL)\n",
        "aggr_fn = partial(np.mean, axis=0)\n",
        "# Build the datasets for classical model\n",
        "train_sources = list(set(df_train[\"FONTE\"]))\n",
        "X_train, y_train = build_articles_embeddings_data(df_train, train_sources, sentence_embedder_model, aggr_fn=aggr_fn)\n",
        "X_test, y_test = build_articles_embeddings_data(df_test, train_sources, sentence_embedder_model,  aggr_fn=aggr_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzMmIZupHKOU"
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7f6DN-YEhOY"
      },
      "source": [
        "weights = compute_class_weight(class_weight = 'balanced', classes = [0.0, 1.0], y = y_train)\n",
        "class_weights = {0 : weights[0], 1: weights[1]}\n",
        "print(f\"Class weights are {class_weights}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brJlQ54mEbIa"
      },
      "source": [
        "classifier_models_list = {\n",
        "    \"svm\": LinearSVC(class_weight=class_weights),\n",
        "    \"logistic\": LogisticRegression(class_weight=class_weights),\n",
        "    \"random-forest\": RandomForestClassifier(class_weight=class_weights),\n",
        "    \"decision-tree\": DecisionTreeClassifier(class_weight=class_weights)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Vz3vpRXasS"
      },
      "source": [
        "for model_name, classifier_model in classifier_models_list.items():\n",
        "    classifier_model.fit(X_train, y_train)\n",
        "    print(f\"Results for model {model_name}\")\n",
        "    print(classification_report(y_test, classifier_model.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrJ_t1kd02Qf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}