{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document_Subjectivity_Experiment_3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "cNvVyxgEa56v"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ12kxS5ZK3D"
      },
      "source": [
        "Experiment 3\n",
        "\n",
        "Perform document classification based on document embeddings, by finetuning the model first.\n",
        "\n",
        "We can use the already pretrained document embedder, pool the results, and attach a logistic regression/svm on top of it. This way we can obtain a backpropable model. If we want to try other approaches such as decision trees or random forests, we can detach the logistic regression/svm head, and redo the training as in experiment 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNvVyxgEa56v"
      },
      "source": [
        "# Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW4ai4Yea8Fi"
      },
      "source": [
        "!git clone https://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:x-oauth-basic@github.com/alexpod1000/Document-Subjectivity.git\n",
        "%cd Document-Subjectivity/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNyz0em8ao7M"
      },
      "source": [
        "# get the dataset folder\n",
        "!git clone https://github.com/francescoantici/SubjectivITA.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gvjT0dpLCQW"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP4ne673awgC"
      },
      "source": [
        "# Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0jjaqB5yId5"
      },
      "source": [
        "POSSIBLE_EMBEDDERS = [\n",
        "    \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    \"paraphrase-multilingual-mpnet-base-v2\",\n",
        "    \"distiluse-base-multilingual-cased-v1\"\n",
        "]\n",
        "SENTENCE_EMBEDDER_MODEL = POSSIBLE_EMBEDDERS[0]\n",
        "GRADIENT_ACCUMULATION = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayuHthdvcdCc"
      },
      "source": [
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "# Sklearn imports\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5kuP2Ca3Xzi"
      },
      "source": [
        "def get_sentences(split):\n",
        "    \"\"\"\n",
        "    Given a split (train, val, test), load the appropriate sentences file.\n",
        "\n",
        "    Args:\n",
        "        split (str): string indicating the split.\n",
        "    Returns:\n",
        "        df: dataframe with rows representing sentences from articles from\n",
        "        the given split.\n",
        "    \"\"\"\n",
        "    to_keep = [\"ID_ARTICOLO\", \"ID_FRASE\", \"FRASE\", \"TAG_FRASE\", \"TAG_ARTICOLO\", \"FONTE\"]\n",
        "    tag_mapper = {\"SOG\": 0, \"OGG\": 1}\n",
        "    df = pd.read_csv(\n",
        "        \"SubjectivITA/datasets/sentences/sentences{}.csv\".format(split.capitalize())\n",
        "    )\n",
        "    df[\"TAG_FRASE\"] = df[\"TAG_FRASE\"].replace(tag_mapper)\n",
        "    df[\"TAG_ARTICOLO\"] = df[\"TAG_ARTICOLO\"].replace(tag_mapper)\n",
        "    df[\"FONTE\"] = df[\"FONTE\"].astype('category').cat.codes\n",
        "    return df[to_keep]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmZyGjPwDPR0"
      },
      "source": [
        "def load_split_ids(split):\n",
        "    \"\"\"\n",
        "    Given a split (train, test), loads all the article ids for that split.\n",
        "\n",
        "    Args:\n",
        "        split (str): string indicating the split\n",
        "    Returns:\n",
        "        article_ids (List[str]): list containing article ids for the split.\n",
        "    \"\"\"\n",
        "    df_articles = pd.read_csv(\n",
        "        \"SubjectivITA/datasets/articles/articles{}.csv\".format(split.capitalize())\n",
        "    )\n",
        "    df_articles[\"ID_ARTICOLO\"] = df_articles[\"ID_ARTICOLO\"].astype(np.int32)\n",
        "    return df_articles[\"ID_ARTICOLO\"].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxqJ5WK4DGux"
      },
      "source": [
        "# load the article ids for train and test splits\n",
        "articles_ids_train = load_split_ids(\"train\")\n",
        "articles_ids_test = load_split_ids(\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsugc53VE9hU"
      },
      "source": [
        "def get_statistical_data(df, train_sources=None):\n",
        "    \"\"\"\n",
        "    Given an article dataframe, return a list containing its statistical features.\n",
        "    All the article sources that weren't in the initial training dataset will be\n",
        "    mapped to an \"unknown\" source.\n",
        "    \"\"\"\n",
        "    obj_ratio = sum(df[df[\"TAG_FRASE\"]==1][\"TAG_FRASE\"])/len(df)\n",
        "    if train_sources is not None:\n",
        "        # map source to a one hot\n",
        "        source_id = list(df[\"FONTE\"])[0]\n",
        "        if source_id not in train_sources:\n",
        "            source_id = 0\n",
        "        else:\n",
        "            # we assume \"missing\" category to be at index 0\n",
        "            source_id = train_sources.index(source_id) + 1\n",
        "        source_encoding = np.zeros(len(train_sources) + 1, dtype=np.float32)\n",
        "        source_encoding[source_id] = 1.0\n",
        "\n",
        "        # add batch dimension\n",
        "        return np.hstack([obj_ratio, source_encoding])\n",
        "    else:\n",
        "        return np.array([obj_ratio])\n",
        "\n",
        "def build_articles_embeddings_data(df, train_sources, model, use_statistical_data = False, aggr_fn = partial(np.mean, axis=0)):\n",
        "    \"\"\"\n",
        "    Given a dataframe, a document embedding model and an embedding aggregation\n",
        "    function, returns embeddings and subjectivity label as a numpy array.\n",
        "\n",
        "    Args:\n",
        "        df: dataframe containing sentences and their corresponding article id.\n",
        "        train_sources: ordered set containing article sources for training data.\n",
        "        model: a document embedding model.\n",
        "        aggr_fn: function to combine document embeddings into a single representation.\n",
        "\n",
        "    Returns:\n",
        "        X (np.array): matrix in the shape of (n_articles, emb_dim)\n",
        "        y (np.array): vector of article tag for each article\n",
        "    \"\"\"\n",
        "    # Get all the sentences in the dataframe as a list\n",
        "    sentences_list = df[\"FRASE\"].tolist()\n",
        "    # Embed all the sentences in the dataframe at once and return them as a list\n",
        "    # inside the dataframe.\n",
        "    df[\"EMB_FRASE\"] = model.encode(\n",
        "        sentences_list, convert_to_tensor=True\n",
        "    ).cpu().numpy().tolist()\n",
        "    # Helper dictionary for a pandas dataframe\n",
        "    articles_dataset = {}\n",
        "    # Divide articles by groups\n",
        "    article_groups = df.groupby(['ID_ARTICOLO'])\n",
        "    # For each unique article\n",
        "    for article_group in article_groups:\n",
        "        article_id = article_group[0]\n",
        "        # All the sentences for the given article_id\n",
        "        df_by_article_id = article_group[1]\n",
        "        # Document statistical features\n",
        "        if use_statistical_data:\n",
        "            doc_stat = get_statistical_data(df_by_article_id, train_sources)\n",
        "        else:\n",
        "            doc_stat = []\n",
        "\n",
        "        sentence_embeddings = np.array(df_by_article_id[\"EMB_FRASE\"].tolist())\n",
        "        # Get embeddings for a single document (from sentence embeddings)\n",
        "        document_embedding = aggr_fn(sentence_embeddings)\n",
        "        articles_dataset[article_id] = {\n",
        "            \"document_emb\": np.hstack([document_embedding, doc_stat]), \n",
        "            \"article_tag\": df_by_article_id[\"TAG_ARTICOLO\"].tolist()[0]\n",
        "        }\n",
        "    articles_df = pd.DataFrame(articles_dataset).T\n",
        "    # Convert to numpy\n",
        "    X = np.array(articles_df[\"document_emb\"].tolist(), dtype=np.float32)\n",
        "    y = np.array(articles_df[\"article_tag\"], dtype=np.float32)\n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRfxNqg9tqy7"
      },
      "source": [
        "def batch_to_device(batch, target_device):\n",
        "    \"\"\"\n",
        "    send a pytorch batch to a device (CPU/GPU)\n",
        "    \"\"\"\n",
        "    for key in batch:\n",
        "        if isinstance(batch[key], torch.Tensor):\n",
        "            batch[key] = batch[key].to(target_device)\n",
        "    return batch\n",
        "\n",
        "class FinetuneDocumentEmbedderModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, sentence_embedder_model, aggr=\"mean\"):\n",
        "        super(FinetuneDocumentEmbedderModel, self).__init__()\n",
        "        self.sentence_embedder_model = sentence_embedder_model\n",
        "        if aggr==\"mean\":\n",
        "            self.aggr_fn = partial(torch.mean, axis=0)\n",
        "        self.classification_layer = torch.nn.Linear(\n",
        "            in_features=self.sentence_embedder_model.get_sentence_embedding_dimension(),\n",
        "            out_features=1\n",
        "        )\n",
        "\n",
        "    def forward(self, sentences, train=False):\n",
        "        \"\"\"\n",
        "        Note: this model assumes that sentences come from the same article.\n",
        "        \"\"\"\n",
        "        if train:\n",
        "            self.sentence_embedder_model.train()\n",
        "        else:\n",
        "            self.sentence_embedder_model.eval()\n",
        "        # forward pass on the document embedded\n",
        "        features = self.sentence_embedder_model.tokenize(sentences)\n",
        "        features_device = batch_to_device(features, self.sentence_embedder_model.device)\n",
        "        out_features = self.sentence_embedder_model.forward(features_device)\n",
        "        embeddings = out_features[\"sentence_embedding\"]\n",
        "        # aggregate multiple sentence embeddings into a single one\n",
        "        aggr_embeddings = self.aggr_fn(embeddings)\n",
        "        # apply classification head\n",
        "        logits = self.classification_layer(aggr_embeddings)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69ZJJdrizKXK"
      },
      "source": [
        "def finetune_model_train_epoch(model, loss_fn, optimizer, dataloader, gradient_accumulation=1, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    dataloader: a function generating (sentences:List[str], label:Int) pairs.\n",
        "    \"\"\"\n",
        "    stats_collector = {\"loss\": []}\n",
        "    current_step = 0\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    for id, sentences, label_gt in dataloader:\n",
        "        current_step += 1\n",
        "        # prepare label\n",
        "        label_gt = torch.Tensor([label_gt]).to(device)\n",
        "        # forward pass\n",
        "        label_logits = model(sentences)\n",
        "        # compute loss\n",
        "        loss_val = loss_fn(label_logits, label_gt)\n",
        "        if gradient_accumulation > 1:\n",
        "            # normalize loss (assume averaging)\n",
        "            loss_val = loss_val / gradient_accumulation\n",
        "            loss_val.backward()\n",
        "            if current_step % gradient_accumulation == 0:\n",
        "                optimizer.step()\n",
        "                model.zero_grad()\n",
        "        else:\n",
        "            loss_val.backward()\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "        stats_collector[\"loss\"].append(loss_val.item())\n",
        "    return stats_collector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOlJBUoW03gq"
      },
      "source": [
        "import random\n",
        "\n",
        "def finetune_dataloader_fn(df):\n",
        "    # Divide articles by groups\n",
        "    article_groups = df.groupby(['ID_ARTICOLO'])\n",
        "    # we'll avoid using generator to easily shuffle samples\n",
        "    samples = []\n",
        "    # For each unique article\n",
        "    for article_group in article_groups:\n",
        "        article_id = article_group[0]\n",
        "        # All the sentences for the given article_id\n",
        "        df_by_article_id = article_group[1]\n",
        "        article_tag = df_by_article_id[\"TAG_ARTICOLO\"].tolist()[0]\n",
        "        #yield article_id, df_by_article_id[\"FRASE\"].tolist(), article_tag\n",
        "        samples.append((article_id, df_by_article_id[\"FRASE\"].tolist(), article_tag))\n",
        "    random.shuffle(samples)\n",
        "    return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JntbWoeG_ZA"
      },
      "source": [
        "# Load the sentence dataframe for each split\n",
        "df_train = get_sentences(\"train\")\n",
        "df_val = get_sentences(\"val\")\n",
        "df_test = get_sentences(\"test\")\n",
        "# Combine all the splits into a single dataframe\n",
        "df_all = pd.concat([df_train, df_val, df_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU-hfQaDFMX0"
      },
      "source": [
        "# redistribute data according to articles splits\n",
        "df_train = df_all.query('ID_ARTICOLO in @articles_ids_train')\n",
        "df_test = df_all.query('ID_ARTICOLO in @articles_ids_test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NvJDrf37YSY"
      },
      "source": [
        "Note: Adam and AdamW were tried, but resulted in huge overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_FKOPvht5Z5"
      },
      "source": [
        "def prepare_data_for_classical(df_train, df_test, sentence_embedder_model):\n",
        "    aggr_fn = partial(np.mean, axis=0)\n",
        "    # Build the datasets for classical model\n",
        "    train_sources = list(set(df_train[\"FONTE\"]))\n",
        "    use_statistical_data = False\n",
        "    X_train, y_train = build_articles_embeddings_data(df_train, train_sources, sentence_embedder_model, use_statistical_data=use_statistical_data, aggr_fn=aggr_fn)\n",
        "    X_test, y_test = build_articles_embeddings_data(df_test, train_sources, sentence_embedder_model, use_statistical_data=use_statistical_data, aggr_fn=aggr_fn)\n",
        "    print(X_train.shape, X_test.shape)\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpEeSyFK8oS-"
      },
      "source": [
        "def train_classifical_classifiers(X_train, y_train, X_test, y_test):\n",
        "    weights = compute_class_weight(class_weight = 'balanced', classes = [0.0, 1.0], y = y_train)\n",
        "    class_weights = {0 : weights[0], 1: weights[1]}\n",
        "    print(f\"Class weights are {class_weights}\")\n",
        "\n",
        "    classifier_models_list = {\n",
        "        \"svm\": LinearSVC(class_weight=class_weights),\n",
        "        \"logistic\": LogisticRegression(class_weight=class_weights),\n",
        "        \"random-forest\": RandomForestClassifier(class_weight=class_weights),\n",
        "        \"decision-tree\": DecisionTreeClassifier(class_weight=class_weights)\n",
        "    }\n",
        "\n",
        "    for model_name, classifier_model in classifier_models_list.items():\n",
        "        classifier_model.fit(X_train, y_train)\n",
        "        print(f\"Results for model {model_name}\")\n",
        "        print(classification_report(y_test, classifier_model.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR0neMNave3g"
      },
      "source": [
        "# Get document embedder\n",
        "sentence_embedder_model = SentenceTransformer(SENTENCE_EMBEDDER_MODEL)\n",
        "# NOTE: sentence_embedder_model model works only on cuda (apparently)\n",
        "finetune_model = FinetuneDocumentEmbedderModel(sentence_embedder_model).to(\"cuda\")\n",
        "#optimizer = torch.optim.SGD(finetune_model.parameters(), lr=0.001) #\n",
        "optimizer = torch.optim.AdamW(finetune_model.parameters(), lr=0.0001, weight_decay=0.001)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fBIOBcAutk1"
      },
      "source": [
        "print(\"Results BEFORE finetuning\")\n",
        "train_classifical_classifiers(*prepare_data_for_classical(df_train, df_test, sentence_embedder_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkYRz2HH37V6"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "for epoch in range(N_EPOCHS):\n",
        "    stats = finetune_model_train_epoch(\n",
        "        finetune_model, loss_fn, optimizer, \n",
        "        finetune_dataloader_fn(df_train),\n",
        "        gradient_accumulation=GRADIENT_ACCUMULATION\n",
        "    )\n",
        "    print(f\"Epoch {epoch+1}, train_loss_avg: {np.mean(stats['loss'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLN3bra73uKq"
      },
      "source": [
        "We can now exploit our code from Experiment 1 and Experiment 2 but with finetuned **sentence_embedder_model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrJ_t1kd02Qf"
      },
      "source": [
        "print(\"Results AFTER finetuning\")\n",
        "train_classifical_classifiers(*prepare_data_for_classical(df_train, df_test, sentence_embedder_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vxPS_0PtqgY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}