{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Document-Subjectivity-Playground",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNvVyxgEa56v"
      },
      "source": [
        "# Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW4ai4Yea8Fi"
      },
      "source": [
        "!git clone https://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:x-oauth-basic@github.com/alexpod1000/Document-Subjectivity.git\n",
        "%cd Document-Subjectivity/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gvjT0dpLCQW"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP4ne673awgC"
      },
      "source": [
        "# Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNyz0em8ao7M"
      },
      "source": [
        "!git clone https://github.com/francescoantici/SubjectivITA.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayuHthdvcdCc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5KvYCYIDk_b"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdOQ-9CNDlU8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFBertModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "maxSentenceLen = 20\n",
        "\n",
        "def prepare_data(X, y):\n",
        "    pad = tf.keras.preprocessing.sequence.pad_sequences#(seq, padding = 'post', maxlen = maxlen)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
        "    dataFields = {\n",
        "            \"input_ids\": [],\n",
        "            \"token_type_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"emotions\": []\n",
        "        }\n",
        "    lbls = {\n",
        "        'SOG' : 1.0,\n",
        "        'OGG' : 0.0\n",
        "    }\n",
        "    for i in range(len(X)):\n",
        "        data = tokenizer(X[i])\n",
        "        padded = pad([data['input_ids'], data['attention_mask'], data['token_type_ids']], padding = 'post', maxlen = maxSentenceLen)\n",
        "        dataFields['input_ids'].append(padded[0])\n",
        "        dataFields['attention_mask'].append(padded[1])\n",
        "        dataFields['token_type_ids'].append(padded[-1])\n",
        "        dataFields['emotions'].append(lbls[y[i]])\n",
        "    \n",
        "    for key in dataFields:\n",
        "        dataFields[key] = np.array(dataFields[key])\n",
        "    \n",
        "    return [dataFields[\"input_ids\"], dataFields[\"token_type_ids\"], dataFields[\"attention_mask\"]], dataFields[\"emotions\"]\n",
        "\n",
        "def create_sentences_model(useAlberto = False):\n",
        "    input_ids = tf.keras.layers.Input(shape=(maxSentenceLen,), dtype=tf.int32)\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(maxSentenceLen,), dtype=tf.int32)\n",
        "    attention_mask = tf.keras.layers.Input(shape=(maxSentenceLen,), dtype=tf.int32)\n",
        "    bertModel = TFBertModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[-1]\n",
        "    out = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(tf.keras.layers.Dropout(0.1)(bertModel))\n",
        "    model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n",
        "    model.compile(optimizer = tf.optimizers.Adam(1e-5), loss = tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_sentences_model(model, Xtrain, ytrain, validation_data, save_weights = True):\n",
        "  try:\n",
        "    Xtrain, ytrain = prepare_data(Xtrain, ytrain)\n",
        "    weights = compute_class_weight(class_weight = 'balanced', classes = [0.0, 1.0], y = ytrain)\n",
        "    class_weights = {0 : weights[0], 1: weights[1]}\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max', patience = 2, restore_best_weights = True)\n",
        "    model.fit(Xtrain, ytrain, validation_data = prepare_data(validation_data[0], validation_data[1]), batch_size = 16, epochs = 4, callbacks = [callback], class_weight = class_weights)\n",
        "    if save_weights:\n",
        "      model.save_weights('weights/sentencesModelWeights.h5')\n",
        "    print(\"Sentences model trained successfully!\")\n",
        "    return model\n",
        "  except Exception as e:\n",
        "    print(\"Error in training sentences model: {}\".format(e))\n",
        "    return False\n",
        "\n",
        "def evaluate_sentences_model(model, Xtest, ytest):\n",
        "  Xtest, ytest = prepare_data(Xtest, ytest)\n",
        "  y_pred = model.predict(Xtest)\n",
        "  ypred = toLabels(y_pred)\n",
        "  ytest = toLabels(ytest)\n",
        "  with open('results/reports_sentences.txt', 'w') as f:\n",
        "    f.write(classification_report(ytest, y_pred = ypred)+ \"\\n\")\n",
        "\n",
        "def get_sentences(split):\n",
        "  df = pd.read_csv(\"SubjectivITA/datasets/sentences/sentences{}.csv\".format(split.capitalize()))\n",
        "  return df['FRASE'].values, df['TAG_FRASE'].values\n",
        "  \n",
        "def toLabels(data, subT = 0.5):\n",
        "    ypred = []\n",
        "    for pred in data:\n",
        "        if pred >= subT:\n",
        "            ypred.append('SOG')\n",
        "        else:\n",
        "            ypred.append('OGG')\n",
        "    return ypred\n",
        "\n",
        "def main(train = False):\n",
        "  sentencesModel = create_sentences_model()\n",
        "  sentencesXtrain, sentencesytrain = get_sentences(split = 'train')\n",
        "  sentencesXval, sentencesyval = get_sentences(split = 'val')\n",
        "  sentencesXtest, sentencesytest = get_sentences(split = 'test')\n",
        "  if train:\n",
        "    sentencesModel = train_sentences_model(sentencesModel, sentencesXtrain, sentencesytrain, validation_data = (sentencesXval, sentencesyval))\n",
        "  else:\n",
        "    try:\n",
        "      sentencesModel.load_weights('weights/sentencesModelWeights.h5')\n",
        "    except:\n",
        "      print(\"No weights found!\")\n",
        "  evaluate_sentences_model(sentencesModel, sentencesXtest, sentencesytest)\n",
        "\n",
        "main(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv_q9ZK4IISD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHTcmGE4IIb4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rW277xbIIeg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch0wTigGIIho"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "def create_articles_model(modelName = 'random-forest'):\n",
        "  modelSwitcher = {\n",
        "      \"svm\" : LinearSVC,\n",
        "      \"logistic\" : LogisticRegression,\n",
        "      \"decision-tree\" : DecisionTreeClassifier,\n",
        "      \"random-forest\": RandomForestClassifier,\n",
        "      \"naive-bayes\" : MultinomialNB\n",
        "  }\n",
        "  return modelSwitcher[modelName]()\n",
        "\n",
        "def train_articles_model(model, Xtrain, ytrain):\n",
        "  try:\n",
        "    model.fit(X = Xtrain, y = ytrain)\n",
        "    print(\"Articles model trained successfully!\")\n",
        "    return model\n",
        "  except Exception as e:\n",
        "    print(\"Error in training articles model: {}\".format(e))\n",
        "    return False\n",
        "\n",
        "def evaluate_articles_model(model, Xtest, ytest):\n",
        "  y_pred = model.predict(Xtest)\n",
        "  toLabel = {1:\"SOG\", 0:\"OGG\"}\n",
        "  ypred = list(map(lambda x: toLabel[x], y_pred))\n",
        "  ytest = list(map(lambda x: toLabel[x], ytest))\n",
        "  with open('results/reports_articles.txt', 'a') as f:\n",
        "    f.write(classification_report(ytest, y_pred = ypred)+ \"\\n\")\n",
        "\n",
        "def get_articles(split):\n",
        "  df = pd.read_csv(\"SubjectivITA/datasets/articles/articles{}.csv\".format(split.capitalize()))\n",
        "  fonti = list(df['FONTE'].unique())\n",
        "  tags = {\"OGG\" : 0, \"SOG\" : 1}\n",
        "  X = df.drop(['ID_ARTICOLO', 'TAG_ARTICOLO'], axis = 1)\n",
        "  X['FONTE'] = X['FONTE'].map(lambda x: fonti.index(x))\n",
        "  X['FRASI_SOG'] = X['FRASI_SOG']/X['FRASI']\n",
        "  X['FRASI_OGG'] = X['FRASI_OGG']/X['FRASI']\n",
        "  X = X.drop(['FRASI'], axis = 1)\n",
        "  #scaler = MinMaxScaler()\n",
        "  #scaler = scaler.fit(X['FONTE'].values.reshape(-1, 1))\n",
        "  #X['FONTE'] = scaler.transform(X['FONTE'].values.reshape(-1, 1))\n",
        "  print(X)\n",
        "  y = np.array(list(map(lambda x: tags[x], df['TAG_ARTICOLO'].values)))\n",
        "  return X, y\n",
        "\n",
        "def main():\n",
        "    articlesXtrain, articlesytrain = get_articles(split='train')\n",
        "    articlesXTest, articlesytest = get_articles(split='test')\n",
        "    for model in [\"svm\", \"logistic\", \"random-forest\", \"naive-bayes\", \"decision-tree\"]:\n",
        "      articlesModel = create_articles_model(model)\n",
        "      articlesModel = train_articles_model(articlesModel, articlesXtrain, articlesytrain)\n",
        "      evaluate_articles_model(articlesModel, articlesXTest, articlesytest)\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQdPKMzISkpw"
      },
      "source": [
        "\"\"\"\n",
        "OUTLINE\n",
        "\n",
        "1. creare un dataset pyt a cui passiamo il dataframe, e che ci sputa fuori frasi come stringhe oppure già elaborate da un tokenizzatore\n",
        "2. \n",
        "\n",
        "OUTLINE (per il task più semplice, pretrained model doc embeddings + modello ml classico):\n",
        "1. passare tutte le stringhe dei documenti ad una funzione che tira fuori doc per doc gli embeddings di tutte le frasi.\n",
        "2. trovare una strategia per mergere insieme gli embeddings nei singoli doc (mean, lstm, attention(?)).\n",
        "3. fittare sopra questa rappresentazione, un modello di ML classico (https://bytepawn.com/svm-with-pytorch.html, deeplearning.net/wp-content/uploads/2013/03/dlsvm.pdf).\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5kuP2Ca3Xzi"
      },
      "source": [
        "def get_sentences(split):\n",
        "    to_keep = [\"ID_ARTICOLO\", \"ID_FRASE\", \"FRASE\", \"TAG_FRASE\", \"TAG_ARTICOLO\", \"FONTE\"]\n",
        "    tag_mapper = {\"SOG\": 0, \"OGG\": 1}\n",
        "    df = pd.read_csv(\"SubjectivITA/datasets/sentences/sentences{}.csv\".format(split.capitalize()))\n",
        "    df[\"TAG_FRASE\"] = df[\"TAG_FRASE\"].replace(tag_mapper)\n",
        "    df[\"TAG_ARTICOLO\"] = df[\"TAG_ARTICOLO\"].replace(tag_mapper)\n",
        "    # TODO(alexo): check better if different splits have overlapping ids for different FONTE field\n",
        "    df[\"FONTE\"] = df[\"FONTE\"].astype('category').cat.codes\n",
        "    return df[to_keep]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmZyGjPwDPR0"
      },
      "source": [
        "def load_split_ids(split):\n",
        "    df_articles = pd.read_csv(\"SubjectivITA/datasets/articles/articles{}.csv\".format(split.capitalize()))\n",
        "    df_articles[\"ID_ARTICOLO\"] = df_articles[\"ID_ARTICOLO\"].astype(np.int32)\n",
        "    return df_articles[\"ID_ARTICOLO\"].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxqJ5WK4DGux"
      },
      "source": [
        "articles_ids_train = load_split_ids(\"train\")\n",
        "articles_ids_test = load_split_ids(\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsugc53VE9hU"
      },
      "source": [
        "def build_articles_embeddings_data(df, model, use_sentences_stats=True, aggr_fn = partial(np.mean, axis=0)):\n",
        "    sentences_list = df[\"FRASE\"].tolist()\n",
        "    df[\"EMB_FRASE\"] = model.encode(sentences_list, convert_to_tensor=True).cpu().numpy().tolist()\n",
        "    articles_dataset = {}\n",
        "    # Divide articles by groups\n",
        "    article_groups = df.groupby(['ID_ARTICOLO'])\n",
        "    for article_group in article_groups:\n",
        "        article_id = article_group[0]\n",
        "        df_by_article_id = article_group[1]\n",
        "        if use_sentences_stats:\n",
        "            sentences_with_tag_1 = sum(df_by_article_id[\"TAG_FRASE\"].tolist())\n",
        "            total_sentences = len(df_by_article_id)\n",
        "        sentence_embeddings = np.array(df_by_article_id[\"EMB_FRASE\"].tolist())\n",
        "        # Get embeddings for a single document (from sentence embeddings)\n",
        "        document_embedding = aggr_fn(sentence_embeddings)\n",
        "        if use_sentences_stats:\n",
        "            articles_dataset[article_id] = {\"sent_tag_ratio\": [sentences_with_tag_1/total_sentences], \"document_emb\": document_embedding, \"article_tag\": df_by_article_id[\"TAG_ARTICOLO\"].tolist()[0]}\n",
        "        else:\n",
        "            articles_dataset[article_id] = {\"document_emb\": document_embedding, \"article_tag\": df_by_article_id[\"TAG_ARTICOLO\"].tolist()[0]}\n",
        "    articles_df = pd.DataFrame(articles_dataset).T\n",
        "    # convert to numpy\n",
        "    if use_sentences_stats:\n",
        "        X_sent_feat = np.array(articles_df[\"sent_tag_ratio\"].tolist(), dtype=np.float32)\n",
        "        #X = np.array(articles_df[\"document_emb\"].tolist(), dtype=np.float32)\n",
        "        #X = np.concatenate([X, X_sent_feat], axis=-1)\n",
        "        X = X_sent_feat\n",
        "    else:\n",
        "        X = np.array(articles_df[\"document_emb\"].tolist(), dtype=np.float32)\n",
        "    y = np.array(articles_df[\"article_tag\"], dtype=np.float32)\n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JntbWoeG_ZA"
      },
      "source": [
        "df_train = get_sentences(\"train\")\n",
        "df_val = get_sentences(\"val\")\n",
        "df_test = get_sentences(\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buYx4AD4Ezip"
      },
      "source": [
        "df_all = pd.concat([df_train, df_val, df_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU-hfQaDFMX0"
      },
      "source": [
        "df_train = df_all.query('ID_ARTICOLO in @articles_ids_train')\n",
        "df_test = df_all.query('ID_ARTICOLO in @articles_ids_test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVxXKhrjGISX"
      },
      "source": [
        "X_train, y_train = build_articles_embeddings_data(df_train, model)\n",
        "#X_val, y_val = build_articles_embeddings_data(df_val, model)\n",
        "X_test, y_test = build_articles_embeddings_data(df_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzMmIZupHKOU"
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7f6DN-YEhOY"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "weights = compute_class_weight(class_weight = 'balanced', classes = [0.0, 1.0], y = y_train)\n",
        "class_weights = {0 : weights[0], 1: weights[1]}    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "107kcKJZIBTH"
      },
      "source": [
        "class_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brJlQ54mEbIa"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "basic_model = LinearSVC()#class_weight=class_weights)\n",
        "#basic_model = LogisticRegression(class_weight=class_weights)\n",
        "basic_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDW1ji-WHuOp"
      },
      "source": [
        "print(\"TRAIN SET\")\n",
        "print(classification_report(y_train, basic_model.predict(X_train)))\n",
        "#print(\"VAL SET\")\n",
        "#print(classification_report(y_val, basic_model.predict(X_val)))\n",
        "print(\"TEST SET\")\n",
        "print(classification_report(y_test, basic_model.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJjBZ9kIH6E0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYZ5lMFF6Fy0"
      },
      "source": [
        "import torch\n",
        "\n",
        "class ArticleSentencesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.df.iloc[idx]\n",
        "        return {\n",
        "            \"article_id\": sample[\"ID_ARTICOLO\"], \n",
        "            \"sentence_id\": sample[\"ID_FRASE\"], \n",
        "            \"sentence\": sample[\"FRASE\"], \n",
        "            \"sentence_tag\": sample[\"TAG_FRASE\"], \n",
        "            \"article_tag\": sample[\"TAG_ARTICOLO\"]\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6aiQ3yz7wsW"
      },
      "source": [
        "ddd = ArticleSentencesDataset(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcFOxNxl74zE"
      },
      "source": [
        "def article_sentences_collate_fn(sample_list):\n",
        "    # NOTE: the tokenizer in dataloader already pads inputs to have same length of 384\n",
        "    input_ids_padded = [sample[\"input_ids\"] for sample in sample_list]\n",
        "    attention_mask_padded = [sample[\"attention_mask\"] for sample in sample_list]\n",
        "    out = [sample[\"out_span\"] for sample in sample_list]\n",
        "    # Convert inputs to Torch tensors\n",
        "    input_ids_padded = torch.tensor(input_ids_padded, dtype=torch.long)\n",
        "    attention_mask_padded = torch.tensor(attention_mask_padded, dtype=torch.long)\n",
        "    # Tensor adds an extra dimension, so remove it\n",
        "    input_ids_padded = input_ids_padded[:, 0, :]\n",
        "    attention_mask_padded = attention_mask_padded[:, 0, :]\n",
        "    return {\"input_ids\": input_ids_padded,\n",
        "            \"attention_mask\": attention_mask_padded,\n",
        "            \"y_gt\":torch.stack(out),\n",
        "            \"paragraph_id\":paragraph_id,\n",
        "            \"question_id\":question_id}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq4YDNh1LRjf"
      },
      "source": [
        "#model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Two lists of sentences\n",
        "#\"\"\"\n",
        "sentences1 = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'The new movie is awesome']\n",
        "\n",
        "sentences2 = ['The dog plays in the garden',\n",
        "              'A woman watches TV',\n",
        "              'The new movie is so great']\n",
        "#\"\"\"\n",
        "\"\"\"sentences1 = ['Il nostro prodotto è bello',\n",
        "             'Il gatto attraversa la strada']\n",
        "\n",
        "sentences2 = ['Questo prodotto è stupendo',\n",
        "              'Un animale attraversa la strada']\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarits\n",
        "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences1)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9W0kVdi1PTr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFXizY-VL4FX"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMmhLtpBLtla"
      },
      "source": [
        "# Two lists of sentences\n",
        "sentences1 = ['The animal is running in the woods',\n",
        "             'A man is playing guitar',\n",
        "             'The new movie is awesome']\n",
        "\n",
        "sentences2 = ['A big cat is in the garden',\n",
        "              'A person is using an instrument',\n",
        "              'The new movie is so great']\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "# let's test a \"readout\" like operation\n",
        "embeddings_single_1 = torch.mean(embeddings1, axis=0)\n",
        "embeddings_single_2 = torch.mean(embeddings2, axis=0)\n",
        "\n",
        "cosine_single_scores = util.pytorch_cos_sim(embeddings_single_1, embeddings_single_2)\n",
        "\n",
        "print(cosine_single_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INnmut3ZMCXv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mworaThMII-"
      },
      "source": [
        "cosine_single_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nae01ZanLc5p"
      },
      "source": [
        "embeddings1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yNwC1mXI27J"
      },
      "source": [
        "Progress/Notes list:\n",
        "\n",
        "- Italian BERT can be used still in PyT for example.\n",
        "- TODO: try PyT Lighting if switching to PyT\n",
        "- Removed FONTE as a predictor feature. It makes no sense to treat such a categorical feature in a numeric way.\n",
        "- TODO: try using FONTE embedded by a neural network (MLP from n_FONTE to an embedding dim)"
      ]
    }
  ]
}